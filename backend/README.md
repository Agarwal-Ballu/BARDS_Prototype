This is the backend service for the Boosted AI Driven Refactoring System (Hybrid model code generator/debugger).
It powers the VS Code extension by generating, debugging, and evaluating Python code using a hybrid AI approach:

Gemini â†’ reasoning, planning, validation

CodeLlama (via Ollama) â†’ final code generation (local)

The backend runs locally using FastAPI and does not expose API keys in code.

-------------------------------------------------------------------------------------------------------------------

ðŸš€ Features

âœ… Hybrid LLM pipeline (Gemini + CodeLlama)
âœ… Python-only code generation & debugging
âœ… Optional auto-test generation
âœ… AI accuracy scoring
âœ… Guardrails to reject non-Python requests
âœ… Environment-based secret management
âœ… FastAPI + async execution


--------------------------------------------------------------------------------------------------------------------

ðŸ§° Prerequisites

Make sure you have:
---> Python 3.9+
---> Ollama installed and running
---> CodeLlama pulled locally:
        ollama pull codellama
---> A valid Gemini API key

Installation :-
cd backend
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt

-----------------------------------------------------------------------------------------------------------------------

â–¶ï¸ Run the Server

uvicorn app.main:app --reload
Api docs available at - http://127.0.0.1:8000/docs


ðŸ›¡ï¸ Guardrails

âŒ Non-Python questions are rejected
âŒ Explanations removed when not requested
âœ… Code-only output enforced
âœ… Safe fallback to CodeLlama if Gemini quota fails

------------------------------------------------------------------------------------------------------------------------

Flow :-

 User Prompt -->  Language Guard (Python only) --> Gemini (planning / validation) (fallback if quota fails) --> CodeLlama (code generation) --> Accuracy Scoring --> VS Code Extension
